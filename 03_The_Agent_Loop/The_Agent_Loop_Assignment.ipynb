{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Agent Loop: Building Production Agents with LangChain 1.0\n",
    "\n",
    "> **Note:** While this notebook can be adapted to use various LLM providers, we'll be using the Anthropic Claude API. Please follow the best practices outlined in the [SRHG AI Usage Guidelines](https://srhg.enterprise.slack.com/docs/T0HANKTEC/F0AB86J3A1L).\n",
    "\n",
    "In this notebook, we'll explore the foundational concepts of AI agents and learn how to build production-grade agents using LangChain's new `create_agent` abstraction with middleware support. We'll build a **Stone Ridge Investment Assistant** that can answer questions about Stone Ridge's investment philosophy, market insights, and strategic outlook.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand what an \"agent\" is and how the agent loop works\n",
    "- Learn the core constructs of LangChain (Runnables, LCEL)\n",
    "- Master the `create_agent` function and middleware system\n",
    "- Build an agentic RAG application using Qdrant for Stone Ridge investor letters\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "- **Part 1:** Introduction to LangChain, LangSmith, and `create_agent`\n",
    "  - Task 1: Dependencies\n",
    "  - Task 2: Environment Variables\n",
    "  - Task 3: LangChain Core Concepts (Runnables & LCEL)\n",
    "  - Task 4: Understanding the Agent Loop\n",
    "  - Task 5: Building Your First Agent with `create_agent()`\n",
    "  - Question #1 & Question #2\n",
    "  - Activity #1: Create a Custom Tool\n",
    "\n",
    "- **Part 2:** Middleware - Agentic RAG with Qdrant\n",
    "  - Task 6: Loading & Chunking Documents\n",
    "  - Task 7: Setting up Qdrant Vector Database\n",
    "  - Task 8: Creating a RAG Tool\n",
    "  - Task 9: Introduction to Middleware\n",
    "  - Task 10: Building Agentic RAG with Middleware\n",
    "  - Question #3 & Question #4\n",
    "  - Activity #2: Enhance the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1\n",
    "## Introduction to LangChain, LangSmith, and `create_agent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Dependencies\n",
    "\n",
    "First, let's ensure we have all the required packages installed. We'll be using:\n",
    "\n",
    "- **LangChain 1.0+**: The core framework with the new `create_agent` API\n",
    "- **LangChain-Anthropic**: Anthropic Claude model integrations\n",
    "- **LangChain-OpenAI**: OpenAI embeddings (we'll use Claude for chat, OpenAI for embeddings)\n",
    "- **LangSmith**: Observability and tracing\n",
    "- **Qdrant**: Vector database for RAG\n",
    "- **PyMuPDF**: PDF parsing for investor letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to install dependencies (if not using uv sync)\n",
    "# !pip install langchain>=1.0.0 langchain-anthropic langchain-openai langsmith langgraph qdrant-client langchain-qdrant pymupdf nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports we'll use throughout the notebook\n",
    "import os\n",
    "import getpass\n",
    "from uuid import uuid4\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Required for async operations in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Environment Variables\n",
    "\n",
    "We need to set up our API keys for:\n",
    "1. **Anthropic** - For Claude models (chat/reasoning)\n",
    "2. **OpenAI** - For embeddings (text-embedding-3-small)\n",
    "3. **LangSmith** - For tracing and observability (optional but recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Anthropic API Key (for Claude chat models)\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Anthropic API Key: \")\n",
    "\n",
    "# Set OpenAI API Key (for embeddings)\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key (for embeddings): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith tracing enabled. Project: AIE9 - The Agent Loop - d7c43c3d\n"
     ]
    }
   ],
   "source": [
    "# Optional: Set up LangSmith for tracing\n",
    "# This provides powerful debugging and observability for your agents\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE9 - The Agent Loop - {uuid4().hex[0:8]}\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key (press Enter to skip): \") or \"\"\n",
    "\n",
    "if not os.environ[\"LANGCHAIN_API_KEY\"]:\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "    print(\"LangSmith tracing disabled\")\n",
    "else:\n",
    "    print(f\"LangSmith tracing enabled. Project: {os.environ['LANGCHAIN_PROJECT']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: LangChain Core Concepts\n",
    "\n",
    "Before diving into agents, let's understand the fundamental building blocks of LangChain.\n",
    "\n",
    "### What is a Runnable?\n",
    "\n",
    "A **Runnable** is the core abstraction in LangChain - think of it as a standardized component that:\n",
    "- Takes an input\n",
    "- Performs some operation\n",
    "- Returns an output\n",
    "\n",
    "Every component in LangChain (models, prompts, retrievers, parsers) is a Runnable, which means they all share the same interface:\n",
    "\n",
    "```python\n",
    "result = runnable.invoke(input)           # Single input\n",
    "results = runnable.batch([input1, input2]) # Multiple inputs\n",
    "for chunk in runnable.stream(input):       # Streaming\n",
    "    print(chunk)\n",
    "```\n",
    "\n",
    "### What is LCEL (LangChain Expression Language)?\n",
    "\n",
    "**LCEL** allows you to chain Runnables together using the `|` (pipe) operator:\n",
    "\n",
    "```python\n",
    "chain = prompt | model | output_parser\n",
    "result = chain.invoke({\"query\": \"Hello!\"})\n",
    "```\n",
    "\n",
    "This is similar to Unix pipes - the output of one component becomes the input to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see LCEL in action with a simple example\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create our components (each is a Runnable)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that speaks like a pirate.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-sonnet-4-20250514\", temperature=0.7)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Chain them together with LCEL\n",
    "pirate_chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, matey! The capital of France be Paris, that grand port city on the Seine River! 'Tis a fine place with many a treasure, from the towering Eiffel Tower to the grand Louvre where they keep their most prized plunder. Aye, Paris be the heart of that landlubber nation, savvy?\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain\n",
    "response = pirate_chain.invoke({\"question\": \"What is the capital of France?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Understanding the Agent Loop\n",
    "\n",
    "### What is an Agent?\n",
    "\n",
    "An **agent** is a system that uses an LLM to decide what actions to take. Unlike a simple chain that follows a fixed sequence, an agent can:\n",
    "\n",
    "1. **Reason** about what to do next\n",
    "2. **Take actions** by calling tools\n",
    "3. **Observe** the results\n",
    "4. **Iterate** until the task is complete\n",
    "\n",
    "### The Agent Loop\n",
    "\n",
    "The core of every agent is the **agent loop**:\n",
    "\n",
    "```\n",
    "                          AGENT LOOP                         \n",
    "                                                             \n",
    "      +----------+     +----------+     +----------+         \n",
    "      |  Model   | --> |   Tool   | --> |  Model   | --> ... \n",
    "      |   Call   |     |   Call   |     |   Call   |         \n",
    "      +----------+     +----------+     +----------+         \n",
    "           |                                  |              \n",
    "           v                                  v              \n",
    "      \"Use search\"                   \"Here's the answer\"     \n",
    "```\n",
    "\n",
    "1. **Model Call**: The LLM receives the current state and decides whether to:\n",
    "   - Call a tool (continue the loop)\n",
    "   - Return a final answer (exit the loop)\n",
    "\n",
    "2. **Tool Call**: If the model decides to use a tool, the tool is executed and its output is added to the conversation\n",
    "\n",
    "3. **Repeat**: The loop continues until the model decides it has enough information to answer\n",
    "\n",
    "### Why `create_agent`?\n",
    "\n",
    "LangChain 1.0 introduced `create_agent` as the new standard way to build agents. It provides:\n",
    "\n",
    "- **Simplified API**: One function to create production-ready agents\n",
    "- **Middleware Support**: Hook into any point in the agent loop\n",
    "- **Built on LangGraph**: Uses the battle-tested LangGraph runtime under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Building Your First Agent with `create_agent()`\n",
    "\n",
    "Let's build a simple agent that can perform calculations and tell the time.\n",
    "\n",
    "### Step 1: Define Tools\n",
    "\n",
    "Tools are functions that the agent can call. We use the `@tool` decorator to create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools created:\n",
      "  - calculate: Evaluate a mathematical expression. Use this for any math ca...\n",
      "  - get_current_time: Get the current date and time. Use this when the user asks a...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluate a mathematical expression. Use this for any math calculations.\n",
    "    \n",
    "    Args:\n",
    "        expression: A mathematical expression to evaluate (e.g., '2 + 2', '10 * 5')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Using eval with restricted globals for safety\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return f\"The result of {expression} is {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error evaluating expression: {e}\"\n",
    "\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Get the current date and time. Use this when the user asks about the current time or date.\"\"\"\n",
    "    from datetime import datetime\n",
    "    return f\"The current date and time is: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Create our tool belt\n",
    "tools = [calculate, get_current_time]\n",
    "\n",
    "print(\"Tools created:\")\n",
    "for t in tools:\n",
    "    print(f\"  - {t.name}: {t.description[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Agent\n",
    "\n",
    "Now we use `create_agent` to build our agent. The function takes:\n",
    "- `model`: The LLM to use (can be a string like `\"gpt-5\"` or a model instance)\n",
    "- `tools`: List of tools the agent can use\n",
    "- `prompt`: Optional system prompt to customize behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created successfully!\n",
      "Type: <class 'langgraph.graph.state.CompiledStateGraph'>\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# Create the Claude model for our agent\n",
    "claude_model = ChatAnthropic(model=\"claude-sonnet-4-20250514\", temperature=0)\n",
    "\n",
    "# Create our first agent\n",
    "simple_agent = create_agent(\n",
    "    model=claude_model,\n",
    "    tools=tools,\n",
    "    system_prompt=\"You are a helpful assistant that can perform calculations and tell the time. Always explain your reasoning.\"\n",
    ")\n",
    "\n",
    "print(\"Agent created successfully!\")\n",
    "print(f\"Type: {type(simple_agent)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run the Agent\n",
    "\n",
    "The agent is a Runnable, so we can invoke it like any other LangChain component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Response:\n",
      "The answer is 1,200. When you multiply 25 by 48, you get 1,200.\n"
     ]
    }
   ],
   "source": [
    "# Test the agent with a simple calculation\n",
    "response = simple_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is 25 * 48?\"}]}\n",
    ")\n",
    "\n",
    "# Print the final response\n",
    "print(\"Agent Response:\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Response:\n",
      "The current time is 00:55:14 (12:55:14 AM) on February 10, 2026. The current hour is 0 (midnight hour in 24-hour format).\n",
      "\n",
      "However, I cannot divide 100 by 0 as that would result in division by zero, which is undefined mathematically. \n",
      "\n",
      "If you'd like me to use a different interpretation:\n",
      "- If we consider the hour in 12-hour format, it would be 12 (midnight as hour 12)\n",
      "- Or if you meant to wait until a different hour when division would be possible\n",
      "\n",
      "Would you like me to calculate 100 Ã· 12 instead, or would you prefer to specify a different approach?\n"
     ]
    }
   ],
   "source": [
    "# Test with a multi-step question that requires multiple tool calls\n",
    "response = simple_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What time is it, and what is 100 divided by the current hour?\"}]}\n",
    ")\n",
    "\n",
    "print(\"Agent Response:\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Agent Conversation:\n",
      "==================================================\n",
      "\n",
      "[HUMAN]\n",
      "What time is it, and what is 100 divided by the current hour?\n",
      "\n",
      "[AI]\n",
      "[{'text': \"I'll help you find the current time and then calculate 100 divided by the current hour.\", 'type': 'text'}, {'id': 'toolu_019SAw3Ui4cnqce9QNw5QwLp', 'input': {}, 'name': 'get_current_time', 'type': 'tool_use'}]\n",
      "\n",
      "[TOOL]\n",
      "The current date and time is: 2026-02-10 00:55:14\n",
      "\n",
      "[AI]\n",
      "The current time is 00:55:14 (12:55:14 AM) on February 10, 2026. The current hour is 0 (midnight hour in 24-hour format).\n",
      "\n",
      "However, I cannot divide 100 by 0 as that would result in division by zero, which is undefined mathematically. \n",
      "\n",
      "If you'd like me to use a different interpretation:\n",
      "- If we consider the hour in 12-hour format, it would be 12 (midnight as hour 12)\n",
      "- Or if you meant to wait until a different hour when division would be possible\n",
      "\n",
      "Would you like me to calculate 100 Ã· 12 instead,\n"
     ]
    }
   ],
   "source": [
    "# Let's see the full conversation to understand the agent loop\n",
    "print(\"Full Agent Conversation:\")\n",
    "print(\"=\" * 50)\n",
    "for msg in response[\"messages\"]:\n",
    "    role = msg.type if hasattr(msg, 'type') else 'unknown'\n",
    "    content = msg.content if hasattr(msg, 'content') else str(msg)\n",
    "    print(f\"\\n[{role.upper()}]\")\n",
    "    print(content[:500] if len(str(content)) > 500 else content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Agent Responses\n",
    "\n",
    "For better UX, we can stream the agent's responses as they're generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Agent Response:\n",
      "==================================================\n",
      "\n",
      "[Node: model]\n",
      "[{'text': \"I'll calculate 15% of 250 for you.\", 'type': 'text'}, {'id': 'toolu_01HU7sPshywfRWsuoAFFSshE', 'input': {'expression': '0.15 * 250'}, 'name': 'calculate', 'type': 'tool_use'}]\n",
      "\n",
      "[Node: tools]\n",
      "The result of 0.15 * 250 is 37.5\n",
      "\n",
      "[Node: model]\n",
      "15% of 250 is **37.5**.\n",
      "\n",
      "To explain the calculation: 15% means 15/100 = 0.15, so we multiply 250 by 0.15 to get the result.\n"
     ]
    }
   ],
   "source": [
    "# Stream the agent's response\n",
    "print(\"Streaming Agent Response:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for chunk in simple_agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Calculate 15% of 250\"}]},\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"\\n[Node: {node}]\")\n",
    "        if \"messages\" in values:\n",
    "            for msg in values[\"messages\"]:\n",
    "                if hasattr(msg, 'content') and msg.content:\n",
    "                    print(msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## â“ Question #1:\n",
    "\n",
    "In the agent loop, what determines whether the agent continues to call tools or returns a final answer to the user? How does `create_agent` handle this decision internally?\n",
    "\n",
    "##### âœ… Answer:\n",
    "THe agent will call models and tools until it determines it has the necessary amount of information to answer the question that was posed. The agent makes alternating calls to the LLM and associated tools, building up a list of intermediate messages. If the message replies contain 'tool_call' identifiers, the agent will call those tools and then re-evaulate its current output. If no tool calls are present in the msot recently added message the agent returns the last message in the list as the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â“ Question #2:\n",
    "\n",
    "Looking at the `calculate` and `get_current_time` tools we created, why is the **docstring** so important for each tool? How does the agent use this information when deciding which tool to call?\n",
    "\n",
    "##### âœ… Answer:\n",
    "The docstrings for the tool functions tell the agent what kind of extra capabilities it has available, and what the expected input arguments and output structure of those functional capabilities are. It can deteremine which of its existing tools is most capable of addressing the most recent message in the chain, without having to parse through the actual logic in the tool function itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ—ï¸ Activity #1: Create a Custom Tool\n",
    "\n",
    "Create your own custom tool and add it to the agent! \n",
    "\n",
    "Ideas:\n",
    "- A tool that converts temperatures between Celsius and Fahrenheit\n",
    "- A tool that generates a random number within a range\n",
    "- A tool that counts words in a given text\n",
    "\n",
    "Requirements:\n",
    "1. Use the `@tool` decorator\n",
    "2. Include a clear docstring (this is what the agent sees!)\n",
    "3. Add it to the agent and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Create your custom tool\n",
    "@tool\n",
    "def get_current_timezone() -> str:\n",
    "    \"\"\"Get the current timezone information. Use this when the user asks about the timezone, time zone, or what timezone they are in.\n",
    "    \n",
    "    Returns:\n",
    "        str: The current timezone name and UTC offset\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    \n",
    "    # Get timezone name\n",
    "    tz_name = time.tzname[time.daylight]\n",
    "    \n",
    "    # Get UTC offset\n",
    "    offset_seconds = -time.timezone if not time.daylight else -time.altzone\n",
    "    offset_hours = offset_seconds // 3600\n",
    "    offset_minutes = (abs(offset_seconds) % 3600) // 60\n",
    "    \n",
    "    # Format offset\n",
    "    offset_str = f\"UTC{'+' if offset_hours >= 0 else ''}{offset_hours:02d}:{offset_minutes:02d}\"\n",
    "    \n",
    "    return f\"Current timezone: {tz_name} ({offset_str})\"\n",
    "\n",
    "@tool\n",
    "def get_time_hours_ago(hours: float) -> str:\n",
    "    \"\"\"Calculate what the date and time was a specified number of hours ago. Use this when the user asks about past times.\n",
    "    \n",
    "    Args:\n",
    "        hours: The number of hours to go back in time (e.g., 2.5 for 2.5 hours ago)\n",
    "    \n",
    "    Returns:\n",
    "        str: The date and time that many hours ago\n",
    "    \"\"\"\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    try:\n",
    "        past_time = datetime.now() - timedelta(hours=float(hours))\n",
    "        return f\"{hours} hour(s) ago, it was: {past_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error calculating past time: {e}\"\n",
    "\n",
    "# Add your tools to the tools list\n",
    "tools = [calculate, get_current_time, get_current_timezone, get_time_hours_ago]\n",
    "\n",
    "# Create a new agent with the updated tools\n",
    "simple_agent = create_agent(\n",
    "    model=claude_model,\n",
    "    tools=tools,\n",
    "    system_prompt=\"You are a helpful assistant that can perform time-based calculations, tell the time, provide timezone information, and calculate past times. Always explain your reasoning.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Response:\n",
      "The local time 6 hours ago was **2026-02-09 18:55:23** (6:55:23 PM on February 9th, 2026).\n",
      "\n",
      "This calculation takes into account your current local timezone, so the result shows what your local time was exactly 6 hours before the current moment.\n"
     ]
    }
   ],
   "source": [
    "# Test your custom tool with the agent\n",
    "response = simple_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What was the local time here 6 hours ago?\"}]}\n",
    ")\n",
    "\n",
    "print(\"Agent Response:\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2\n",
    "## Middleware - Agentic RAG with Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the basics of agents, let's build something more powerful: an **Agentic RAG** system.\n",
    "\n",
    "Traditional RAG follows a fixed pattern: retrieve â†’ generate. But **Agentic RAG** gives the agent control over when and how to retrieve information, making it more flexible and intelligent.\n",
    "\n",
    "We'll also introduce **middleware** - hooks that let us customize the agent's behavior at every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Loading & Chunking Documents\n",
    "\n",
    "We'll use the Stone Ridge 2025 Investor Letter - the same document from Module 2 - to build our investment assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s)\n",
      "Total characters: 52,942\n"
     ]
    }
   ],
   "source": [
    "# Load the document using PyMuPDF for PDF parsing\n",
    "from aimakerspace.text_utils import PDFFileLoader, CharacterTextSplitter\n",
    "\n",
    "# Load the Stone Ridge investor letter\n",
    "pdf_loader = PDFFileLoader(\"data/Stone Ridge 2025 Investor Letter.pdf\")\n",
    "documents = pdf_loader.load_documents()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s)\")\n",
    "print(f\"Total characters: {sum(len(doc) for doc in documents):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 133 chunks\n",
      "\n",
      "Sample chunk:\n",
      "--------------------------------------------------\n",
      "2025 Investor Letter\n",
      "Investor Letter\n",
      "â€œEvery driver has a limit.  Mine is a little bit further than others.â€\n",
      "â€”â€‚ Ayrton Senna, greatest Formula One driver of all time\n",
      "â€œIâ€™m not funny.  What I am is brave.â€\n",
      "â€”â€‚ Lucille Ball, greatest female comedian of all time\n",
      "â€œIâ€™d rather be optimistic and wrong than pe...\n"
     ]
    }
   ],
   "source": [
    "# Split the documents into chunks\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_texts(documents)\n",
    "\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(\"-\" * 50)\n",
    "print(chunks[0][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Setting up Qdrant Vector Database\n",
    "\n",
    "Qdrant is a production-ready vector database. We'll use an in-memory instance for development, but the same code works with a hosted Qdrant instance.\n",
    "\n",
    "Key concepts:\n",
    "- **Collection**: A namespace for storing vectors (like a table in SQL)\n",
    "- **Points**: Individual vectors with optional payloads (metadata)\n",
    "- **Distance**: How similarity is measured (we'll use cosine similarity)\n",
    "\n",
    "We'll use OpenAI's `text-embedding-3-small` for embeddings - it provides excellent quality for semantic search over financial documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 1536\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Get embedding dimension\n",
    "sample_embedding = embedding_model.embed_query(\"test\")\n",
    "embedding_dim = len(sample_embedding)\n",
    "print(f\"Embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created collection: investment_knowledge_base\n"
     ]
    }
   ],
   "source": [
    "# Create Qdrant client (in-memory for development)\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "\n",
    "# Create a collection for our investment documents\n",
    "collection_name = \"investment_knowledge_base\"\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=embedding_dim,\n",
    "        distance=Distance.COSINE\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Created collection: {collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 133 documents to vector store\n"
     ]
    }
   ],
   "source": [
    "# Create the vector store and add documents\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Convert chunks to LangChain Document objects\n",
    "langchain_docs = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "# Create vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding_model\n",
    ")\n",
    "\n",
    "# Add documents to the vector store\n",
    "vector_store.add_documents(langchain_docs)\n",
    "\n",
    "print(f\"Added {len(langchain_docs)} documents to vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved documents:\n",
      "\n",
      "--- Document 1 ---\n",
      "n the middle of a meeting with your largest \n",
      "investor?  At Stone Ridge, we do not pull our phones out on each other.\n",
      "With Stone Ridge increasingly Stone Ridgeâ€™s largest investor, the scale of our prof...\n",
      "\n",
      "--- Document 2 ---\n",
      " palpable distinction, and that Stone Ridge feels different inside. \n",
      "Earlier this year, Stone Ridge Holdings Group (SRHG), parent company of Stone Ridge Asset Management, NYDIG, \n",
      "and Longtail Re, comp...\n",
      "\n",
      "--- Document 3 ---\n",
      "trument or to adopt any investment strategy. This communication \n",
      "does not represent valuation judgments with respect to any financial instrument, issuer, security or sector that may be described \n",
      "or r...\n"
     ]
    }
   ],
   "source": [
    "# Test the retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "test_results = retriever.invoke(\"What is Stone Ridge's investment philosophy?\")\n",
    "\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(test_results, 1):\n",
    "    print(f\"\\n--- Document {i} ---\")\n",
    "    print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Creating a RAG Tool\n",
    "\n",
    "Now we'll wrap our retriever as a tool that the agent can use. This is the key to **Agentic RAG** - the agent decides when to retrieve information about Stone Ridge's investment philosophy and strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool created: search_investment_knowledge\n",
      "Description: Search the Stone Ridge investment knowledge base for information about investment philosophy, market...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_investment_knowledge(query: str) -> str:\n",
    "    \"\"\"Search the Stone Ridge investment knowledge base for information about investment philosophy, market insights, and strategic outlook.\n",
    "    \n",
    "    Use this tool when the user asks questions about:\n",
    "    - Stone Ridge's investment philosophy and approach\n",
    "    - Market analysis and insights from investor letters\n",
    "    - Strategic outlook and portfolio positioning\n",
    "    - Company updates and business developments\n",
    "    - Historical performance context and investment themes\n",
    "    \n",
    "    Args:\n",
    "        query: The search query to find relevant investment information\n",
    "    \"\"\"\n",
    "    results = retriever.invoke(query)\n",
    "    \n",
    "    if not results:\n",
    "        return \"No relevant information found in the investment knowledge base.\"\n",
    "    \n",
    "    # Format the results\n",
    "    formatted_results = []\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        formatted_results.append(f\"[Source {i}]:\\n{doc.page_content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_results)\n",
    "\n",
    "print(f\"Tool created: {search_investment_knowledge.name}\")\n",
    "print(f\"Description: {search_investment_knowledge.description[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Introduction to Middleware\n",
    "\n",
    "**Middleware** in LangChain 1.0 allows you to hook into the agent loop at various points:\n",
    "\n",
    "```\n",
    "                       MIDDLEWARE HOOKS                 \n",
    "                                                        \n",
    "   +--------------+                    +--------------+ \n",
    "   | before_model | --> MODEL CALL --> | after_model  | \n",
    "   +--------------+                    +--------------+ \n",
    "                                                        \n",
    "   +-------------------+                                \n",
    "   | wrap_model_call   |  (intercept and modify calls)  \n",
    "   +-------------------+                                \n",
    "```\n",
    "\n",
    "Common use cases:\n",
    "- **Logging**: Track what the agent is doing\n",
    "- **Guardrails**: Filter or modify inputs/outputs\n",
    "- **Rate limiting**: Control API usage\n",
    "- **Human-in-the-loop**: Pause for human approval\n",
    "\n",
    "LangChain provides middleware through **decorator functions** that hook into specific points in the agent loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging middleware created!\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.middleware import before_model, after_model\n",
    "\n",
    "# Track how many model calls we've made\n",
    "model_call_count = 0\n",
    "\n",
    "@before_model\n",
    "def log_before_model(state, runtime):\n",
    "    \"\"\"Called before each model invocation.\"\"\"\n",
    "    global model_call_count\n",
    "    model_call_count += 1\n",
    "    message_count = len(state.get(\"messages\", []))\n",
    "    print(f\"[LOG] Model call #{model_call_count} - Messages in state: {message_count}\")\n",
    "    return None  # Return None to continue without modification\n",
    "\n",
    "@after_model\n",
    "def log_after_model(state, runtime):\n",
    "    \"\"\"Called after each model invocation.\"\"\"\n",
    "    last_message = state.get(\"messages\", [])[-1] if state.get(\"messages\") else None\n",
    "    if last_message:\n",
    "        has_tool_calls = hasattr(last_message, 'tool_calls') and last_message.tool_calls\n",
    "        print(f\"[LOG] After model - Tool calls requested: {has_tool_calls}\")\n",
    "    return None\n",
    "\n",
    "print(\"Logging middleware created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call limit middleware created!\n",
      "  - Thread limit: 10\n",
      "  - Run limit: 5\n"
     ]
    }
   ],
   "source": [
    "# You can also use the built-in ModelCallLimitMiddleware to prevent runaway agents\n",
    "from langchain.agents.middleware import ModelCallLimitMiddleware\n",
    "\n",
    "# This middleware will stop the agent after 10 model calls per thread\n",
    "call_limiter = ModelCallLimitMiddleware(\n",
    "    thread_limit=10,  # Max calls per conversation thread\n",
    "    run_limit=5,      # Max calls per single run\n",
    "    exit_behavior=\"end\"  # What to do when limit is reached\n",
    ")\n",
    "\n",
    "print(\"Call limit middleware created!\")\n",
    "print(f\"  - Thread limit: {call_limiter.thread_limit}\")\n",
    "print(f\"  - Run limit: {call_limiter.run_limit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Building Agentic RAG with Middleware\n",
    "\n",
    "Now let's put it all together: an agentic RAG system with middleware support!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investment Agent created with middleware!\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# Reset the call counter\n",
    "model_call_count = 0\n",
    "\n",
    "# Define our tools - include the RAG tool and the calculator from earlier\n",
    "rag_tools = [\n",
    "    search_investment_knowledge,\n",
    "    calculate,\n",
    "    get_current_time\n",
    "]\n",
    "\n",
    "# Create the Claude model for our RAG agent\n",
    "claude_rag_model = ChatAnthropic(model=\"claude-sonnet-4-20250514\", temperature=0)\n",
    "\n",
    "# Create the agentic RAG system with middleware\n",
    "investment_agent = create_agent(\n",
    "    model=claude_rag_model,\n",
    "    tools=rag_tools,\n",
    "    system_prompt=\"\"\"You are a helpful Stone Ridge investment assistant with access to a comprehensive knowledge base of investor letters and company information.\n",
    "\n",
    "Your role is to:\n",
    "1. Answer questions about Stone Ridge's investment philosophy, market insights, and strategic outlook\n",
    "2. Always search the knowledge base when the user asks investment-related questions\n",
    "3. Provide accurate, helpful information based on the retrieved context\n",
    "4. Be professional and informative in your responses\n",
    "5. If you cannot find relevant information, say so honestly\n",
    "6. Include a reminder that information is for educational purposes only and not investment advice when appropriate\n",
    "\n",
    "Remember: Always cite information from the knowledge base when applicable.\"\"\",\n",
    "    middleware=[\n",
    "        log_before_model,\n",
    "        log_after_model,\n",
    "        call_limiter\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Investment Agent created with middleware!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Investment Agent\n",
      "==================================================\n",
      "[LOG] Model call #1 - Messages in state: 1\n",
      "[LOG] After model - Tool calls requested: [{'name': 'search_investment_knowledge', 'args': {'query': 'Stone Ridge investment philosophy approach strategy'}, 'id': 'toolu_01UmjjJMwG7aufSPJLLXHKek', 'type': 'tool_call'}]\n",
      "[LOG] Model call #2 - Messages in state: 3\n",
      "[LOG] After model - Tool calls requested: [{'name': 'search_investment_knowledge', 'args': {'query': 'Stone Ridge core principles investment beliefs philosophy approach'}, 'id': 'toolu_01Qpe5QjZQ9ty4mt7vnQYqrM', 'type': 'tool_call'}]\n",
      "[LOG] Model call #3 - Messages in state: 5\n",
      "[LOG] After model - Tool calls requested: [{'name': 'search_investment_knowledge', 'args': {'query': 'Stone Ridge investment methodology risk management approach diversification'}, 'id': 'toolu_017EUqpoiNeiFPBcKE6MGQPd', 'type': 'tool_call'}]\n",
      "[LOG] Model call #4 - Messages in state: 7\n",
      "[LOG] After model - Tool calls requested: [{'name': 'search_investment_knowledge', 'args': {'query': 'Stone Ridge mission financial security innovation product development'}, 'id': 'toolu_019i2nRQZBDmdWMeYJbuziNF', 'type': 'tool_call'}]\n",
      "[LOG] Model call #5 - Messages in state: 9\n",
      "[LOG] After model - Tool calls requested: [{'name': 'search_investment_knowledge', 'args': {'query': '\"Stone Ridge\" philosophy approach \"build products we want\" operating model'}, 'id': 'toolu_01CxfDtYKrMJS5tA8RjY8fqy', 'type': 'tool_call'}]\n",
      "[LOG] Model call #6 - Messages in state: 11\n",
      "\n",
      "==================================================\n",
      "FINAL RESPONSE:\n",
      "==================================================\n",
      "Model call limits exceeded: run limit (5/5)\n"
     ]
    }
   ],
   "source": [
    "# Test the investment agent\n",
    "print(\"Testing Investment Agent\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = investment_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is Stone Ridge's investment philosophy?\"}]}\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL RESPONSE:\")\n",
    "print(\"=\" * 50)\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with complex query\n",
      "==================================================\n",
      "[LOG] Model call #7 - Messages in state: 1\n",
      "[LOG] After model - Tool calls requested: [{'name': 'search_investment_knowledge', 'args': {'query': 'energy investments oil gas energy sector strategy'}, 'id': 'toolu_01N41xSgabPAbxnbTvuhjFi3', 'type': 'tool_call'}, {'name': 'calculate', 'args': {'expression': '100000000 * (1 + 0.5)'}, 'id': 'toolu_019aEvRw8fjqnRP1etijsBG7', 'type': 'tool_call'}]\n",
      "[LOG] Model call #8 - Messages in state: 4\n",
      "[LOG] After model - Tool calls requested: [{'name': 'search_investment_knowledge', 'args': {'query': 'Stone Ridge Energy SRE investment philosophy approach natural gas oil'}, 'id': 'toolu_0116y2oaH9GUM3n35yXQ5YxP', 'type': 'tool_call'}]\n",
      "[LOG] Model call #9 - Messages in state: 6\n",
      "[LOG] After model - Tool calls requested: []\n",
      "\n",
      "==================================================\n",
      "FINAL RESPONSE:\n",
      "==================================================\n",
      "Based on the Stone Ridge investment knowledge base, here's what Stone Ridge says about their energy investments:\n",
      "\n",
      "## Stone Ridge Energy (SRE) Investment Approach\n",
      "\n",
      "**Key Investment Philosophy:**\n",
      "- Stone Ridge Energy has purchased almost **$11 billion of energy assets** since inception\n",
      "- All acquisitions are done via **proprietary securitizations** with \"no bankers, no information leakage, no fee leakage\"\n",
      "- They focus on aligning physics and finance, with teams \"crisply aligned on maximizing a singular metric â€“ total dollars of operating profit\"\n",
      "\n",
      "**Performance Results:**\n",
      "- Across thirteen equity tranches, their integrated approach has produced **>20% annual returns with low volatility and no correlation to anything**\n",
      "- This performance was achieved despite natural gas prices fluctuating dramatically from $1.50/MMBtu to almost $9/MMBtu\n",
      "\n",
      "**Strategic Vision:**\n",
      "- Stone Ridge sees SRE becoming \"a different kind of company\" over the next five years\n",
      "- Their goal is to become a **top three U.S. producer of hydrocarbons** through disciplined acquisitions, careful development, and process improvement\n",
      "- They emphasize the civic importance of their work, aiming to meaningfully impact U.S. families, factories, and national competitiveness\n",
      "- More than 99% of their managed capital comes from American investors\n",
      "\n",
      "## Investment Calculation\n",
      "\n",
      "For your second question: If Stone Ridge invested $100 million with a 50% return over 12 years, the total value would be **$150 million**.\n",
      "\n",
      "*Please note: This information is for educational purposes only and should not be considered investment advice. Past performance does not guarantee future results.*\n"
     ]
    }
   ],
   "source": [
    "# Test with a more complex query\n",
    "print(\"Testing with complex query\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = investment_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What does Stone Ridge say about their energy investments? Also, if they invested $100 million with a 50% return over 12 years, what would be the total value?\"}]}\n",
    ")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL RESPONSE:\")\n",
    "print(\"=\" * 50)\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing agent decision-making (should NOT use RAG)\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "FINAL RESPONSE:\n",
      "==================================================\n",
      "125 Ã— 8 = 1,000\n",
      "\n",
      "As a wellness coach, I'd like to add that doing mental math exercises like this can be great for cognitive health! Regular mental calculations help keep your brain sharp and can improve focus and concentration. If you're looking to incorporate more brain-boosting activities into your routine, consider:\n",
      "\n",
      "- Daily math puzzles or sudoku\n",
      "- Learning new skills that challenge your mind\n",
      "- Reading regularly\n",
      "- Playing strategy games\n",
      "- Practicing mindfulness meditation, which also supports cognitive function\n",
      "\n",
      "Is there anything specific about mental wellness or physical health you'd like to discuss?\n"
     ]
    }
   ],
   "source": [
    "# Test the agent's ability to know when NOT to use RAG\n",
    "print(\"Testing agent decision-making (should NOT use RAG)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "wellness_agent = create_agent(\n",
    "    model=claude_rag_model,\n",
    "    tools=rag_tools,\n",
    "    system_prompt=\"\"\"You are a wellness coach. Give useful mental health and physical wellness advice\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "response = wellness_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is 125 * 8?\"}]}\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL RESPONSE:\")\n",
    "print(\"=\" * 50)\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Agent\n",
    "\n",
    "The agent created by `create_agent` is built on LangGraph, so we can visualize its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the agent graph\n",
    "try:\n",
    "    from IPython.display import display, Image\n",
    "    display(Image(wellness_agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")\n",
    "    print(\"\\nAgent structure:\")\n",
    "    print(wellness_agent.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## â“ Question #3:\n",
    "\n",
    "How does **Agentic RAG** differ from traditional RAG? What are the advantages and potential disadvantages of letting the agent decide when to retrieve information from the Stone Ridge investor letters?\n",
    "\n",
    "##### âœ… Answer:\n",
    "Agentic RAG combines the external information retrieval capabilities of traditional RAG with a dynamic reasoning and action loop enabled by agents. Agentic RAG can generate an answer to a given input question in multiple steps, using tool calls and LLM reasoning each time to hone the current output to the format of the expected answer.<br><br>\n",
    "The agentic approach has the advantage of performing multiple rounds of auomtated reasoning between rounds of external infomration acquisition to get answers to complex, multi-part questions, without requirng a human to stitch the output of one tool to the input of another tool or model reply. The agentic approach can also result in unnecessary use of tools, consuming more tokens and prolonging the time taken to answer simple questions. It also requires the agent to re-check all of its available tools at each step, costing time and compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â“ Question #4:\n",
    "\n",
    "Looking at the middleware examples (`log_before_model`, `log_after_model`, and `ModelCallLimitMiddleware`), describe a real-world scenario where middleware would be essential for a production agent. What specific middleware hooks would you use and why?\n",
    "\n",
    "##### âœ… Answer:\n",
    "Middleware is essential in production agents for monitoring, governance, and reliability across several critical scenarios.In regulated industries like healthcare and finance, compliance and audit trail middleware is required to record PII access, tool usage, and data sources while redacting sensitive information from logs, proving what data was accessed and how decisions were made. For reliability, retry-with-backoff middleware handles transient API failures, circuit breaker middleware prevents cascading failures when upstream services are down, and rate limiting middleware respects API quotas to prevent a single complex query from exhausting rate limits and breaking the service for everyone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ—ï¸ Activity #2: Enhance the Agentic RAG System\n",
    "\n",
    "Now it's your turn! Enhance the investment agent by implementing ONE of the following:\n",
    "\n",
    "### Option A: Add a New Tool\n",
    "Create a new tool that the agent can use. Ideas:\n",
    "- A tool that calculates compound annual growth rate (CAGR)\n",
    "- A tool that compares investment returns across different time periods\n",
    "- A tool that formats financial figures with proper notation\n",
    "\n",
    "### Option B: Create Custom Middleware\n",
    "Build middleware that adds new functionality:\n",
    "- Middleware that tracks which tools are used most frequently\n",
    "- Middleware that adds a compliance disclaimer to investment-related responses\n",
    "- Middleware that enforces a response length limit\n",
    "\n",
    "### Option C: Improve the RAG Tool\n",
    "Enhance the retrieval tool:\n",
    "- Add metadata filtering by year or topic\n",
    "- Implement reranking of results for financial relevance\n",
    "- Add source citations with relevance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Implement your enhancement below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your enhanced agent here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "03-the-agent-loop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
